{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from einops import rearrange\n",
    "import rp\n",
    "from rp import as_torch_image, display_image, load_image, torch_resize_image\n",
    "from icecream import ic\n",
    "\n",
    "def unique_pixels(image):\n",
    "    \"\"\"\n",
    "    Find unique pixel values in an image tensor and return their RGB values, counts, and inverse indices.\n",
    "\n",
    "    Args:\n",
    "        image (torch.Tensor): Image tensor of shape [c, h, w], where c is the number of channels (e.g., 3 for RGB),\n",
    "                              h is the height, and w is the width of the image.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing three tensors:\n",
    "            - unique_colors (torch.Tensor): Tensor of shape [u, c] representing the unique RGB values found in the image,\n",
    "                                            where u is the number of unique colors.\n",
    "            - counts (torch.Tensor): Tensor of shape [u] representing the counts of each unique color.\n",
    "            - index_matrix (torch.Tensor): Tensor of shape [h, w] representing the inverse indices of each pixel,\n",
    "                                           mapping each pixel to its corresponding unique color index.\n",
    "    \"\"\"\n",
    "    c, h, w = image.shape\n",
    "\n",
    "    # Rearrange the image tensor from [c, h, w] to [h, w, c] using einops\n",
    "    pixels = rearrange(image, \"c h w -> h w c\")\n",
    "\n",
    "    # Flatten the image tensor to [h*w, c]\n",
    "    flattened_pixels = rearrange(pixels, \"h w c -> (h w) c\")\n",
    "\n",
    "    # Find unique RGB values, counts, and inverse indices\n",
    "    unique_colors, inverse_indices, counts = torch.unique(flattened_pixels, dim=0, return_inverse=True, return_counts=True)\n",
    "\n",
    "    # Get the number of unique indices\n",
    "    u = unique_colors.shape[0]\n",
    "\n",
    "    # Reshape the inverse indices back to the original image dimensions [h, w] using einops\n",
    "    index_matrix = rearrange(inverse_indices, \"(h w) -> h w\", h=h, w=w)\n",
    "\n",
    "    # Assert the shapes of the output tensors\n",
    "    assert unique_colors.shape == (u, c)\n",
    "    assert counts.shape == (u,)\n",
    "    assert index_matrix.shape == (h, w)\n",
    "    assert index_matrix.min() == 0\n",
    "    assert index_matrix.max() == u - 1\n",
    "\n",
    "    return unique_colors, counts, index_matrix\n",
    "\n",
    "\n",
    "def sum_indexed_values(image, index_matrix):\n",
    "    \"\"\"\n",
    "    Sum the values in the CHW image tensor based on the indices specified in the HW index matrix.\n",
    "\n",
    "    Args:\n",
    "        image (torch.Tensor): Image tensor of shape [C, H, W], where C is the number of channels,\n",
    "                              H is the height, and W is the width of the image.\n",
    "        index_matrix (torch.Tensor): Index matrix tensor of shape [H, W] containing indices\n",
    "                                     specifying the mapping of each pixel to its corresponding\n",
    "                                     unique value.\n",
    "                                     Indices range [0, U), where U is the number of unique indices\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Tensor of shape [U, C] representing the sum of values in the image tensor\n",
    "                      based on the indices in the index matrix, where U is the number of unique\n",
    "                      indices in the index matrix.\n",
    "    \"\"\"\n",
    "    c, h, w = image.shape\n",
    "    u = index_matrix.max() + 1\n",
    "\n",
    "    # Rearrange the image tensor from [c, h, w] to [h, w, c] using einops\n",
    "    pixels = rearrange(image, \"c h w -> h w c\")\n",
    "\n",
    "    # Flatten the image tensor to [h*w, c]\n",
    "    flattened_pixels = rearrange(pixels, \"h w c -> (h w) c\")\n",
    "\n",
    "    # Create an output tensor of shape [u, c] initialized with zeros\n",
    "    output = torch.zeros((u, c), dtype=flattened_pixels.dtype, device=flattened_pixels.device)\n",
    "\n",
    "    # Scatter sum the flattened pixel values using the index matrix\n",
    "    output.index_add_(0, index_matrix.view(-1), flattened_pixels)\n",
    "\n",
    "    # Assert the shapes of the input and output tensors\n",
    "    assert image.shape == (c, h, w), f\"Expected image shape: ({c}, {h}, {w}), but got: {image.shape}\"\n",
    "    assert index_matrix.shape == (h, w), f\"Expected index_matrix shape: ({h}, {w}), but got: {index_matrix.shape}\"\n",
    "    assert output.shape == (u, c), f\"Expected output shape: ({u}, {c}), but got: {output.shape}\"\n",
    "\n",
    "    return output\n",
    "\n",
    "def indexed_to_image(index_matrix, unique_colors):\n",
    "    \"\"\"\n",
    "    Create a CHW image tensor from an HW index matrix and a UC unique_colors matrix.\n",
    "\n",
    "    Args:\n",
    "        index_matrix (torch.Tensor): Index matrix tensor of shape [H, W] containing indices\n",
    "                                     specifying the mapping of each pixel to its corresponding\n",
    "                                     unique color.\n",
    "        unique_colors (torch.Tensor): Unique colors matrix tensor of shape [U, C] containing\n",
    "                                      the unique color values, where U is the number of unique\n",
    "                                      colors and C is the number of channels.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Image tensor of shape [C, H, W] representing the reconstructed image\n",
    "                      based on the index matrix and unique colors matrix.\n",
    "    \"\"\"\n",
    "    h, w = index_matrix.shape\n",
    "    u, c = unique_colors.shape\n",
    "\n",
    "    # Assert the shapes of the input tensors\n",
    "    assert index_matrix.max() < u, f\"Index matrix contains indices ({index_matrix.max()}) greater than the number of unique colors ({u})\"\n",
    "\n",
    "    # Gather the colors based on the index matrix\n",
    "    flattened_image = unique_colors[index_matrix.view(-1)]\n",
    "\n",
    "    # Reshape the flattened image to [h, w, c]\n",
    "    image = rearrange(flattened_image, \"(h w) c -> h w c\", h=h, w=w)\n",
    "\n",
    "    # Rearrange the image tensor from [h, w, c] to [c, h, w] using einops\n",
    "    image = rearrange(image, \"h w c -> c h w\")\n",
    "\n",
    "    # Assert the shape of the output tensor\n",
    "    assert image.shape == (c, h, w), f\"Expected image shape: ({c}, {h}, {w}), but got: {image.shape}\"\n",
    "\n",
    "    return image\n",
    "\n",
    "\n",
    "def demo_pixellation_via_proxy():\n",
    "    real_image = as_torch_image(\n",
    "        rp.cv_resize_image(\n",
    "            load_image(\"https://i.natgeofe.com/n/4f5aaece-3300-41a4-b2a8-ed2708a0a27c/domestic-dog_thumb_square.jpg\"),\n",
    "            (512, 512),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    c, h, w = real_image.shape\n",
    "\n",
    "    noise_image = torch.randn(c, h // 4, w // 4)\n",
    "\n",
    "    # Resize noise_image using nearest-neighbor interpolation to match the dimensions of real_image\n",
    "    pixelated_noise_image = torch_resize_image(noise_image, 4, \"nearest\")\n",
    "    assert pixelated_noise_image.shape==(c,h,w)\n",
    "\n",
    "    # Find unique pixel values, their indices, and counts in the pixelated noise image\n",
    "    unique_colors, counts, index_matrix = unique_pixels(pixelated_noise_image)\n",
    "\n",
    "    # Sum the color values from real_image based on the indices of the unique noise pixels\n",
    "    summed_colors = sum_indexed_values(real_image, index_matrix)\n",
    "\n",
    "    # Divide the summed color values by the counts to get the average color for each unique pixel\n",
    "    average_colors = summed_colors / rearrange(counts, \"u -> u 1\")\n",
    "\n",
    "    # Create a new pixelated image using the average colors and the index matrix\n",
    "    pixelated_dog_image = indexed_to_image(index_matrix, average_colors)\n",
    "\n",
    "    display_image(pixelated_dog_image)\n",
    "    \n",
    "def calculate_wave_pattern(h, w, frame):\n",
    "    # Create a grid of coordinates\n",
    "    y, x = torch.meshgrid(torch.arange(h), torch.arange(w))\n",
    "    \n",
    "    # Calculate the distance from the center of the image\n",
    "    center_x, center_y = w // 2, h // 2\n",
    "    dist_from_center = torch.sqrt((x - center_x)**2 + (y - center_y)**2)\n",
    "    \n",
    "    # Calculate the angle from the center of the image\n",
    "    angle_from_center = torch.atan2(y - center_y, x - center_x)\n",
    "    \n",
    "    # Calculate the wave pattern based on the distance and angle\n",
    "    wave_freq = 0.05  # Frequency of the waves\n",
    "    wave_amp = 10.0   # Amplitude of the waves\n",
    "    wave_offset = frame * 0.05  # Offset for animation\n",
    "    \n",
    "    dx = wave_amp * torch.cos(dist_from_center * wave_freq + angle_from_center + wave_offset)\n",
    "    dy = wave_amp * torch.sin(dist_from_center * wave_freq + angle_from_center + wave_offset)\n",
    "    \n",
    "    return dx, dy\n",
    "\n",
    "def starfield_zoom(h, w, frame):\n",
    "    # Create a grid of coordinates\n",
    "    y, x = torch.meshgrid(torch.arange(h), torch.arange(w))\n",
    "    \n",
    "    # Calculate the distance from the center of the image\n",
    "    center_x, center_y = w // 2, h // 2\n",
    "    dist_from_center = torch.sqrt((x - center_x)**2 + (y - center_y)**2)\n",
    "    \n",
    "    # Calculate the angle from the center of the image\n",
    "    angle_from_center = torch.atan2(y - center_y, x - center_x)\n",
    "    \n",
    "    # Calculate the starfield zoom effect\n",
    "    zoom_speed = 0.01  # Speed of the zoom effect\n",
    "    zoom_scale = 1.0 + frame * zoom_speed  # Scale factor for the zoom effect\n",
    "    \n",
    "    # Calculate the displacement based on the distance and angle\n",
    "    dx = dist_from_center * torch.cos(angle_from_center) / zoom_scale\n",
    "    dy = dist_from_center * torch.sin(angle_from_center) / zoom_scale\n",
    "    \n",
    "    return dx, dy\n",
    "\n",
    "def warp_noise(noise, dx, dy, s=4):\n",
    "    #This is *certainly* imperfect. We need to have particle swarm in addition to this.\n",
    "\n",
    "    c, h, w = noise.shape\n",
    "    assert dx.shape==(h,w)\n",
    "    assert dy.shape==(h,w)\n",
    "\n",
    "    #s is scaling factor\n",
    "    hs = h * s\n",
    "    ws = w * s\n",
    "    \n",
    "    #Upscale the warping with linear interpolation. Also scale it appropriately.\n",
    "    up_dx = rp.torch_resize_image(dx[None], (hs, ws), interp=\"bilinear\")[0]\n",
    "    up_dy = rp.torch_resize_image(dy[None], (hs, ws), interp=\"bilinear\")[0]\n",
    "    up_dx *= s\n",
    "    up_dy *= s\n",
    "\n",
    "    up_noise = rp.torch_resize_image(noise, (hs, ws), interp=\"nearest\")\n",
    "    assert up_noise.shape == (c, hs, ws)\n",
    "\n",
    "    up_noise = rp.torch_remap_image(up_noise, up_dx, up_dy, relative=True, interp=\"nearest\", add_alpha_mask=True)\n",
    "    up_noise, alpha = up_noise[:-1], up_noise[-1:]\n",
    "    assert up_noise.shape == (c, hs, ws)\n",
    "    assert alpha.shape == (1, hs, ws)\n",
    "    \n",
    "    # Fill occluded regions with noise...\n",
    "    fill_noise = torch.randn_like(up_noise)\n",
    "    up_noise = up_noise * alpha + fill_noise * (1-alpha)\n",
    "    assert up_noise.shape == (c, hs, ws)\n",
    "\n",
    "    # Find unique pixel values, their indices, and counts in the pixelated noise image\n",
    "    unique_colors, counts, index_matrix = unique_pixels(up_noise)\n",
    "    u = len(unique_colors)\n",
    "    assert unique_colors.shape == (u, c)\n",
    "    assert counts.shape == (u,)\n",
    "    assert index_matrix.max() == u - 1\n",
    "    assert index_matrix.min() == 0\n",
    "    assert index_matrix.shape == (hs, ws)\n",
    "\n",
    "    foreign_noise = torch.randn_like(up_noise)\n",
    "    assert foreign_noise.shape == up_noise.shape == (c, hs, ws)\n",
    "\n",
    "    summed_foreign_noise_colors = sum_indexed_values(foreign_noise, index_matrix)\n",
    "    assert summed_foreign_noise_colors.shape == (u, c)\n",
    "\n",
    "    meaned_foreign_noise_colors = summed_foreign_noise_colors / rearrange(counts, \"u -> u 1\")\n",
    "    assert meaned_foreign_noise_colors.shape == (u, c)\n",
    "\n",
    "    meaned_foreign_noise = indexed_to_image(index_matrix, meaned_foreign_noise_colors)\n",
    "    assert meaned_foreign_noise.shape == (c, hs, ws)\n",
    "\n",
    "    zeroed_foreign_noise = foreign_noise - meaned_foreign_noise\n",
    "    assert zeroed_foreign_noise.shape == (c, hs, ws)\n",
    "\n",
    "    counts_as_colors = rearrange(counts, \"u -> u 1\")\n",
    "    counts_image = indexed_to_image(index_matrix, counts_as_colors)\n",
    "    assert counts_image.shape == (1, hs, ws)\n",
    "\n",
    "    #To upsample noise, we must first divide by the area then add zero-sum-noise\n",
    "    output = up_noise\n",
    "    output = output / counts_image ** .5\n",
    "    output = output + zeroed_foreign_noise\n",
    "\n",
    "    #Now we resample the noise back down again\n",
    "    #PLEASE HOPE AREA DOWNSAMPLING WORKS PROPERLY...UNVERIFIED...I think I remember it not working?\n",
    "    output = rp.torch_resize_image(output, (h, w), interp='area')\n",
    "    output = output * s #Adjust variance by multiplying by sqrt of area, aka sqrt(s*s)=s\n",
    "\n",
    "    return output\n",
    "    \n",
    "def demo_noise_warp():\n",
    "    d=rp.JupyterDisplayChannel()\n",
    "    d.display()\n",
    "    device='cuda'\n",
    "    h=w=256\n",
    "    noise=torch.randn(3,h,w).to(device)\n",
    "    wdx,wdy=calculate_wave_pattern(h,w,frame=0)\n",
    "    sdx,sdy=starfield_zoom(h,w,frame=1)\n",
    "\n",
    "    dx=sdx+2*wdx\n",
    "    dy=sdy+2*wdy\n",
    "    \n",
    "    dx/=dx.max()\n",
    "    dy/=dy.max()\n",
    "    Q=-6\n",
    "    dy*=Q\n",
    "    dx*=Q\n",
    "    dx=dx.to(device)\n",
    "    dy=dy.to(device)\n",
    "    new_noise=noise\n",
    "\n",
    "\n",
    "    for _ in range(10000):\n",
    "        # ic(new_noise.device,dx.device,dy.device)\n",
    "\n",
    "        new_noise=warp_noise(new_noise,dx,dy,2)\n",
    "        # display_image(new_noise)\n",
    "        d.update(rp.as_numpy_image(new_noise/4+.5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_noise_warp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def demo_noise_warp():\n",
    "    d=rp.JupyterDisplayChannel()\n",
    "    d.display()\n",
    "    device='cuda'\n",
    "    h=w=512\n",
    "    noise=torch.randn(3,h,w).to(device)\n",
    "    wdx,wdy=calculate_wave_pattern(h,w,frame=0)\n",
    "    sdx,sdy=starfield_zoom(h,w,frame=1)\n",
    "\n",
    "    dx=sdx+0*wdx\n",
    "    dy=sdy+0*wdy\n",
    "    \n",
    "    dx/=dx.max()\n",
    "    dy/=dy.max()\n",
    "    Q=-6\n",
    "    dy*=Q\n",
    "    dx*=Q\n",
    "    dx=dx.to(device)\n",
    "    dy=dy.to(device)\n",
    "    new_noise=noise\n",
    "\n",
    "    frames=[]\n",
    "    import tqdm\n",
    "    for _ in tqdm.tqdm(range(600)):\n",
    "        # ic(new_noise.device,dx.device,dy.device)\n",
    "\n",
    "        new_noise=warp_noise(new_noise,dx,dy,20)\n",
    "        # display_image(new_noise)\n",
    "        d.update(rp.as_numpy_image(new_noise/4+.5))\n",
    "        frames.append(rp.as_numpy_image(new_noise/6+.5))\n",
    "    return frames\n",
    "    \n",
    "frames=demo_noise_warp()\n",
    "rp.save_video_mp4(frames,'swirl_noise_illusion.mp4',video_bitrate='max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_dir=rp.printed(rp.get_unique_copy_path('flow_noise_warping/outputs/swirly_anim_3'))\n",
    "rp.save_images(frames,rp.make_directory(img_dir)+'/%04i.png',show_progress=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Can run on M1 CPU\n",
    "def webcam_demo():\n",
    "    import cv2\n",
    "    import numpy as np\n",
    "    \n",
    "    def draw_hsv(flow, scale=8):\n",
    "        h, w = flow.shape[:2]\n",
    "        hsv = np.zeros((h, w, 3), dtype=np.uint8)\n",
    "        mag, ang = cv2.cartToPolar(flow[..., 0], flow[..., 1])\n",
    "        hsv[..., 0] = ang * 180 / np.pi / 2\n",
    "        hsv[..., 1] = 255\n",
    "        scaled_mag = mag * scale\n",
    "        scaled_mag = np.clip(scaled_mag, 0, 255)  # Ensure it fits in the value range\n",
    "        hsv[..., 2] = scaled_mag.astype(np.uint8)\n",
    "        bgr = cv2.cvtColor(hsv, cv2.COLOR_HSV2BGR)\n",
    "        return bgr\n",
    "    \n",
    "    def resize_frame(frame, target_height=128):\n",
    "        aspect_ratio = frame.shape[1] / frame.shape[0]\n",
    "        target_width = int(target_height * aspect_ratio)\n",
    "        resized_frame = cv2.resize(frame, (target_width, target_height))\n",
    "        return resized_frame\n",
    "    \n",
    "    def main():\n",
    "        cap = cv2.VideoCapture(0)\n",
    "        ret, prev_frame = cap.read()\n",
    "        prev_frame = resize_frame(prev_frame)\n",
    "        prev_gray = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "        # Initialize DeepFlow Optical Flow\n",
    "        optical_flow = cv2.optflow.createOptFlow_DeepFlow()\n",
    "    \n",
    "    \n",
    "        d=rp.JupyterDisplayChannel()\n",
    "        d.display()\n",
    "        device='cpu'\n",
    "        h,w=get_image_dimensions(prev_frame)\n",
    "        noise=torch.randn(3,h,w).to(device)\n",
    "        wdx,wdy=calculate_wave_pattern(h,w,frame=0)\n",
    "        sdx,sdy=starfield_zoom(h,w,frame=1)\n",
    "    \n",
    "        dx=sdx+2*wdx\n",
    "        dy=sdy+2*wdy\n",
    "        \n",
    "        dx/=dx.max()\n",
    "        dy/=dy.max()\n",
    "        Q=-6\n",
    "        dy*=Q\n",
    "        dx*=Q\n",
    "        dx=dx.to(device)\n",
    "        dy=dy.to(device)\n",
    "        new_noise=noise\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            frame = resize_frame(frame)\n",
    "            frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "            # Compute the optical flow\n",
    "            flow = optical_flow.calc(prev_gray, frame_gray, None)\n",
    "    \n",
    "            # Visualization\n",
    "            flow_bgr = draw_hsv(flow)\n",
    "    \n",
    "            x=flow[:,:,0]\n",
    "            y=flow[:,:,1]\n",
    "    \n",
    "            dx=-torch.Tensor(x)\n",
    "            dy=-torch.Tensor(y)\n",
    "            \n",
    "            # Display the original and flow side by side\n",
    "            combined_img = np.hstack((frame, flow_bgr))\n",
    "            cv2.imshow('Frame and Optical Flow', combined_img)\n",
    "            \n",
    "            prev_gray = frame_gray.copy()\n",
    "    \n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "    \n",
    "            new_noise=warp_noise(new_noise,dx,dy,1)\n",
    "            display_image(\n",
    "                tiled_images(\n",
    "                    [\n",
    "                        as_numpy_image(new_noise / 2 + 0.5),\n",
    "                        cv_bgr_rgb_swap(frame),\n",
    "                        cv_bgr_rgb_swap(flow_bgr),\n",
    "                    ]\n",
    "                )\n",
    "            )\n",
    "    \n",
    "            # d.update(rp.as_numpy_image(new_noise/4+.5))\n",
    "    \n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "    \n",
    "    main()\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python vaetuner",
   "language": "python",
   "name": "vaetuner"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
